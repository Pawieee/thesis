{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8466f1",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# BASELINE DENSENET121 K-FOLD CROSS-VALIDATION (GENUINE VS FORGED)\n",
    "# =============================================================================\n",
    "Load pretrained baseline weights and perform 5-fold cross-validation for binary classification.\n",
    "Uses 110 evaluation users split into 5 folds (same as meta-learning).\n",
    "Requires: baseline_pretrain.pth from baseline_pretraining.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dafd3b8",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# STEP 1: SETUP, IMPORTS, AND REPRODUCIBILITY\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e80f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Set repo root\n",
    "current_dir = os.path.abspath(os.getcwd())\n",
    "REPO_ROOT = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "if REPO_ROOT not in sys.path:\n",
    "    sys.path.append(REPO_ROOT)\n",
    "\n",
    "# Import Custom Modules\n",
    "from models.feature_extractor import DenseNetFeatureExtractor\n",
    "from utils.model_evaluation import compute_metrics\n",
    "\n",
    "# Deterministic Seeding for Reproducible Research\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\" > [System] Seed set to: {seed}\")\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# Device Configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\" > [System] Computation Device: {DEVICE}\")\n",
    "print(f\" > [System] CUDA Available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb68005",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# STEP 2: HYPERPARAMETER CONFIGURATION\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9320edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Input Configuration ---\n",
    "IMG_SIZE = 224\n",
    "INPUT_SHAPE = (IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-5  # Lower learning rate for fine-tuning pretrained model\n",
    "EPOCHS = 50\n",
    "N_FOLDS = 5\n",
    "\n",
    "# --- Data Configuration ---\n",
    "# Use 110 evaluation users split into 5 folds (same as meta-learning)\n",
    "SPLIT_DIR = os.path.join(REPO_ROOT, 'data', 'splits')\n",
    "\n",
    "# --- Pretrained Weights ---\n",
    "PRETRAINED_WEIGHTS_PATH = os.path.join(REPO_ROOT, 'baseline_pretrain.pth')\n",
    "\n",
    "# --- Checkpoint Configuration ---\n",
    "CHECKPOINT_DIR = os.path.join(REPO_ROOT, 'checkpoints', 'baseline_kfold')\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"[Config] Image Size: {INPUT_SHAPE}\")\n",
    "print(f\"[Config] Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"[Config] Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"[Config] Epochs: {EPOCHS}\")\n",
    "print(f\"[Config] K-Folds: {N_FOLDS}\")\n",
    "print(f\"[Config] Pretrained Weights: {PRETRAINED_WEIGHTS_PATH}\")\n",
    "print(f\"[Config] Split Directory: {SPLIT_DIR}\")\n",
    "print(f\"[Config] Checkpoints: {CHECKPOINT_DIR}\")\n",
    "\n",
    "# Verify pretrained weights exist\n",
    "if not os.path.exists(PRETRAINED_WEIGHTS_PATH):\n",
    "    print(f\"\\nWARNING: Pretrained weights not found at {PRETRAINED_WEIGHTS_PATH}\")\n",
    "    print(f\"Please run baseline_pretraining.ipynb first!\")\n",
    "else:\n",
    "    print(f\"\\n > Pretrained weights found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36330b5a",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# STEP 3: VERIFY FOLD FILES EXIST\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894e8faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify fold files exist (generated by restructure_bhsig.py in baseline_pretraining.ipynb)\n",
    "print(f\" > [Info] Checking for fold split files in: {SPLIT_DIR}\")\n",
    "\n",
    "fold_files_found = True\n",
    "for fold in range(N_FOLDS):\n",
    "    fold_file = os.path.join(SPLIT_DIR, f'bhsig_meta_split_fold_{fold}.json')\n",
    "    if not os.path.exists(fold_file):\n",
    "        print(f\"ERROR: Fold file not found: {fold_file}\")\n",
    "        fold_files_found = False\n",
    "\n",
    "if fold_files_found:\n",
    "    print(f\" > [Info] All {N_FOLDS} fold files found!\")\n",
    "    print(f\" > [Info] Using 110 evaluation users (same as meta-training)\")\n",
    "else:\n",
    "    print(f\"\\nERROR: Some fold files are missing!\")\n",
    "    print(f\"Please run baseline_pretraining.ipynb first to generate the fold splits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d9a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATASET CLASS FOR GENUINE VS FORGED CLASSIFICATION\n",
    "# =============================================================================\n",
    "\n",
    "class BHSigDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Genuine vs Forged Classification (Binary Classification)\n",
    "    Label 0: Genuine signature\n",
    "    Label 1: Forged signature\n",
    "    \"\"\"\n",
    "    def __init__(self, user_dict, user_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_dict: Dictionary of users with genuine/forged image paths\n",
    "            user_list: List of user IDs to use\n",
    "            transform: Image transformation pipeline\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        \n",
    "        # Collect all samples with binary labels (0=genuine, 1=forged)\n",
    "        for uid in user_list:\n",
    "            if uid in user_dict:\n",
    "                user_data = user_dict[uid]\n",
    "                \n",
    "                # Add genuine samples (label = 0)\n",
    "                for img_path in user_data['genuine']:\n",
    "                    self.samples.append((img_path, 0))\n",
    "                \n",
    "                # Add forged samples (label = 1)\n",
    "                for img_path in user_data['forged']:\n",
    "                    self.samples.append((img_path, 1))\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        # Count samples per class\n",
    "        num_genuine = sum(1 for _, label in self.samples if label == 0)\n",
    "        num_forged = sum(1 for _, label in self.samples if label == 1)\n",
    "        \n",
    "        print(f\"   Dataset initialized: {len(self.samples)} total samples\")\n",
    "        print(f\"   - Genuine: {num_genuine} samples (label=0)\")\n",
    "        print(f\"   - Forged: {num_forged} samples (label=1)\")\n",
    "        print(f\"   - From {len(user_list)} users\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a blank tensor on error\n",
    "            img = torch.zeros(3, IMG_SIZE, IMG_SIZE)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "# =============================================================================\n",
    "# DATA TRANSFORMATIONS\n",
    "# =============================================================================\n",
    "\n",
    "# Training transformations with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(INPUT_SHAPE),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1),\n",
    "        scale=(0.9, 1.1),\n",
    "        fill=0\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation transformations without augmentation\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(INPUT_SHAPE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\" > [Info] Dataset class and transformations defined for binary classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf7aef8",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# STEP 4: MODEL INITIALIZATION WITH PRETRAINED WEIGHTS\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f095faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_with_pretrained(pretrained_path, device):\n",
    "    \"\"\"\n",
    "    Initialize binary classification model and load pretrained weights.\n",
    "    Uses 2 classes: 0=Genuine, 1=Forged\n",
    "    \n",
    "    Args:\n",
    "        pretrained_path: Path to pretrained weights\n",
    "        device: PyTorch device\n",
    "    \n",
    "    Returns:\n",
    "        Model with loaded pretrained backbone\n",
    "    \"\"\"\n",
    "    # Binary classification: 2 classes (genuine vs forged)\n",
    "    num_classes = 2\n",
    "    \n",
    "    # Initialize model\n",
    "    model = DenseNetFeatureExtractor(\n",
    "        backbone_name='densenet121',\n",
    "        output_dim=num_classes,\n",
    "        pretrained=True,\n",
    "        baseline=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load pretrained weights\n",
    "    if os.path.exists(pretrained_path):\n",
    "        print(f\"   Loading pretrained weights from {os.path.basename(pretrained_path)}...\")\n",
    "        checkpoint = torch.load(pretrained_path, map_location=device, weights_only=False)\n",
    "        \n",
    "        # Extract state dict\n",
    "        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "            pretrained_state = checkpoint['model_state_dict']\n",
    "        else:\n",
    "            pretrained_state = checkpoint\n",
    "        \n",
    "        # Load weights directly (both should have 2 classes)\n",
    "        try:\n",
    "            model.load_state_dict(pretrained_state, strict=True)\n",
    "            print(f\"   Successfully loaded all pretrained weights!\")\n",
    "        except RuntimeError as e:\n",
    "            # If there's a mismatch, load only backbone weights\n",
    "            print(f\"   Note: Loading backbone only (classifier may differ)\")\n",
    "            model_state = model.state_dict()\n",
    "            pretrained_state_filtered = {\n",
    "                k: v for k, v in pretrained_state.items() \n",
    "                if k in model_state and model_state[k].shape == v.shape\n",
    "            }\n",
    "            model_state.update(pretrained_state_filtered)\n",
    "            model.load_state_dict(model_state)\n",
    "            print(f\"   Loaded {len(pretrained_state_filtered)} layers\")\n",
    "    else:\n",
    "        print(f\"   WARNING: Pretrained weights not found. Using ImageNet initialization.\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"   Model: DenseNet121 Baseline (Binary Classification)\")\n",
    "    print(f\"   Total Parameters: {total_params:,}\")\n",
    "    print(f\"   Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"   Output Classes: {num_classes} (0=Genuine, 1=Forged)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\" > [Info] Model initialization function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286a956",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# STEP 5: TRAINING UTILITIES\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f977d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        dataloader: Training data loader\n",
    "        optimizer: Optimizer\n",
    "        criterion: Loss function\n",
    "        device: PyTorch device\n",
    "    \n",
    "    Returns:\n",
    "        avg_loss: Average training loss\n",
    "        avg_acc: Average training accuracy\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    avg_acc = correct / total if total > 0 else 0.0\n",
    "    \n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate model and compute comprehensive metrics including EER\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        dataloader: Validation data loader\n",
    "        criterion: Loss function\n",
    "        device: PyTorch device\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Dictionary containing loss, accuracy, precision, recall, f1, EER, AUC\n",
    "        all_preds: All predictions\n",
    "        all_labels: All ground truth labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_scores = []  # Probability scores for EER calculation\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Get probability scores for class 1 (forged)\n",
    "            # For EER calculation, we need soft scores\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]  # Probability of being forged\n",
    "            \n",
    "            all_scores.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    \n",
    "    # Use compute_metrics to get comprehensive evaluation (including EER and AUC)\n",
    "    metrics = compute_metrics(all_labels, all_scores)\n",
    "    \n",
    "    # Add loss to metrics\n",
    "    metrics['loss'] = avg_loss\n",
    "    \n",
    "    # Get hard predictions for compatibility\n",
    "    all_preds = [1 if score > 0.5 else 0 for score in all_scores]\n",
    "    \n",
    "    return metrics, all_preds, all_labels\n",
    "\n",
    "print(\" > [Info] Training functions defined with EER support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c739a06",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# STEP 6: K-FOLD CROSS-VALIDATION EXECUTION\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a9965",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Starting {N_FOLDS}-Fold Cross-Validation with Pretrained Weights\")\n",
    "print(f\"Binary Classification: Genuine (0) vs Forged (1)\")\n",
    "print(f\"Pretrained Weights: {os.path.basename(PRETRAINED_WEIGHTS_PATH)}\")\n",
    "print(f\"Using 110 Evaluation Users\")\n",
    "print(f\"Checkpoint Directory: {CHECKPOINT_DIR}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "# Iterate through the 5 pre-generated fold files\n",
    "for fold in range(N_FOLDS):\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"FOLD {fold + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    \n",
    "    # 1. LOAD FOLD DATA\n",
    "    fold_file = os.path.join(SPLIT_DIR, f'bhsig_meta_split_fold_{fold}.json')\n",
    "    print(f\" > Loading fold file: {os.path.basename(fold_file)}\")\n",
    "    \n",
    "    with open(fold_file, 'r') as f:\n",
    "        fold_data = json.load(f)\n",
    "    \n",
    "    # meta-train: users for training in this fold\n",
    "    # meta-test: users for validation in this fold\n",
    "    train_users = list(fold_data['meta-train'].keys())\n",
    "    val_users = list(fold_data['meta-test'].keys())\n",
    "    \n",
    "    print(f\"   Train users: {len(train_users)}\")\n",
    "    print(f\"   Val users: {len(val_users)}\")\n",
    "    \n",
    "    # 2. DATALOADER SETUP\n",
    "    print(\" > Loading training dataset...\")\n",
    "    train_dataset = BHSigDataset(fold_data['meta-train'], train_users, transform=train_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    print(\" > Loading validation dataset...\")\n",
    "    val_dataset = BHSigDataset(fold_data['meta-test'], val_users, transform=val_transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # 3. MODEL INITIALIZATION WITH PRETRAINED WEIGHTS\n",
    "    model = initialize_model_with_pretrained(PRETRAINED_WEIGHTS_PATH, DEVICE)\n",
    "    \n",
    "    # Optimizer and loss\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=False)\n",
    "    \n",
    "    # 4. TRAINING LOOP (MINIMIZE EER)\n",
    "    best_eer = 1.0\n",
    "    best_acc = 0.0\n",
    "    best_metrics = {}\n",
    "    best_epoch = 0\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        \n",
    "        # Validation (Calculate EER and Accuracy)\n",
    "        val_metrics, _, _ = validate_epoch(model, val_loader, criterion, DEVICE)\n",
    "        val_eer = val_metrics['eer']\n",
    "        val_acc = val_metrics['accuracy']\n",
    "        \n",
    "        # Logging\n",
    "        print(f\"Epoch {epoch+1:03d} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2%} | \"\n",
    "              f\"Val EER: {val_eer:.2%} | Val Acc: {val_acc:.2%}\")\n",
    "        \n",
    "        scheduler.step(val_eer)\n",
    "        \n",
    "        # Save Best Model Logic (Minimize EER)\n",
    "        if val_eer < best_eer:\n",
    "            best_eer = val_eer\n",
    "            best_acc = val_acc\n",
    "            best_metrics = val_metrics\n",
    "            best_epoch = epoch\n",
    "\n",
    "            # Save Checkpoint\n",
    "            ckpt_path = os.path.join(CHECKPOINT_DIR, f\"best_model_fold_{fold}.pth\")\n",
    "            torch.save({\n",
    "                'fold': fold,\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'num_classes': 2,  # Binary classification\n",
    "                'val_eer': val_eer,\n",
    "                'val_acc': val_acc,\n",
    "                'metrics': best_metrics\n",
    "            }, ckpt_path)\n",
    "            print(f\"   >>> Best Model Saved! (EER: {val_eer:.2%} | ACC: {val_acc:.2%})\")\n",
    "        \n",
    "        elif val_eer == best_eer and val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_metrics = val_metrics\n",
    "            best_epoch = epoch\n",
    "            \n",
    "            # Save Checkpoint\n",
    "            ckpt_path = os.path.join(CHECKPOINT_DIR, f\"best_model_fold_{fold}.pth\")\n",
    "            torch.save({\n",
    "                'fold': fold,\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'num_classes': 2,  # Binary classification\n",
    "                'val_eer': val_eer,\n",
    "                'val_acc': val_acc,\n",
    "                'metrics': best_metrics\n",
    "            }, ckpt_path)\n",
    "            print(f\"   >>> Best Model Saved! (EER: {val_eer:.2%} | ACC: {val_acc:.2%})\")\n",
    "    \n",
    "    # 5. FOLD SUMMARY\n",
    "    print(f\"\\nFold {fold+1} Summary:\")\n",
    "    print(f\"   Best Epoch: {best_epoch+1}\")\n",
    "    print(f\"   Best Val EER: {best_eer:.2%}\")\n",
    "    print(f\"   Best Val Accuracy: {best_acc:.2%}\")\n",
    "    print(f\"   Best Val AUC: {best_metrics['auc']:.4f}\")\n",
    "    print(f\"   Best Val Precision: {best_metrics['precision']:.4f}\")\n",
    "    print(f\"   Best Val Recall: {best_metrics['recall']:.4f}\")\n",
    "    print(f\"   Best Val F1: {best_metrics['f1']:.4f}\")\n",
    "\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    fold_results.append(best_metrics)\n",
    "    \n",
    "print(f\"All Folds Training Complete\")\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747238ae",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# STEP 7: FINAL CROSS-VALIDATION RESULTS\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389d6106",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"{'FINAL CROSS-VALIDATION RESULTS':^60}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Aggregate metrics across folds\n",
    "avg_metrics = {\n",
    "    'eer': [],\n",
    "    'accuracy': [],\n",
    "    'auc': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': [],\n",
    "    'loss': []\n",
    "}\n",
    "\n",
    "for i, result in enumerate(fold_results):\n",
    "    print(f\"\\nFold {i+1}:\")\n",
    "    print(f\"   EER:       {result['eer']:.4f} ({result['eer']*100:.2f}%)\")\n",
    "    print(f\"   Accuracy:  {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)\")\n",
    "    print(f\"   AUC:       {result['auc']:.4f}\")\n",
    "    print(f\"   Precision: {result['precision']:.4f}\")\n",
    "    print(f\"   Recall:    {result['recall']:.4f}\")\n",
    "    print(f\"   F1-Score:  {result['f1']:.4f}\")\n",
    "    print(f\"   Val Loss:  {result['loss']:.4f}\")\n",
    "    \n",
    "    for key in avg_metrics.keys():\n",
    "        avg_metrics[key].append(result[key])\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "\n",
    "# Calculate mean and std\n",
    "mean_eer = np.mean(avg_metrics['eer'])\n",
    "std_eer = np.std(avg_metrics['eer'])\n",
    "\n",
    "mean_acc = np.mean(avg_metrics['accuracy'])\n",
    "std_acc = np.std(avg_metrics['accuracy'])\n",
    "\n",
    "mean_auc = np.mean(avg_metrics['auc'])\n",
    "std_auc = np.std(avg_metrics['auc'])\n",
    "\n",
    "mean_prec = np.mean(avg_metrics['precision'])\n",
    "std_prec = np.std(avg_metrics['precision'])\n",
    "\n",
    "mean_rec = np.mean(avg_metrics['recall'])\n",
    "std_rec = np.std(avg_metrics['recall'])\n",
    "\n",
    "mean_f1 = np.mean(avg_metrics['f1'])\n",
    "std_f1 = np.std(avg_metrics['f1'])\n",
    "\n",
    "mean_loss = np.mean(avg_metrics['loss'])\n",
    "std_loss = np.std(avg_metrics['loss'])\n",
    "\n",
    "print(f\"\\nCROSS-VALIDATION SUMMARY (Mean ± Std):\")\n",
    "print(f\"   EER:       {mean_eer:.4f} ± {std_eer:.4f}  ({mean_eer*100:.2f}% ± {std_eer*100:.2f}%)\")\n",
    "print(f\"   Accuracy:  {mean_acc:.4f} ± {std_acc:.4f}  ({mean_acc*100:.2f}% ± {std_acc*100:.2f}%)\")\n",
    "print(f\"   AUC:       {mean_auc:.4f} ± {std_auc:.4f}\")\n",
    "print(f\"   Precision: {mean_prec:.4f} ± {std_prec:.4f}\")\n",
    "print(f\"   Recall:    {mean_rec:.4f} ± {std_rec:.4f}\")\n",
    "print(f\"   F1-Score:  {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "print(f\"   Val Loss:  {mean_loss:.4f} ± {std_loss:.4f}\")\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save results to JSON\n",
    "results_file = os.path.join(CHECKPOINT_DIR, 'cross_validation_results.json')\n",
    "cv_results = {\n",
    "    'model': 'DenseNet121_Baseline_Binary_Classification',\n",
    "    'task': 'Genuine vs Forged Detection',\n",
    "    'num_classes': 2,\n",
    "    'pretrained_weights': os.path.basename(PRETRAINED_WEIGHTS_PATH),\n",
    "    'config': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'epochs': EPOCHS,\n",
    "        'folds': N_FOLDS,\n",
    "        'img_size': INPUT_SHAPE\n",
    "    },\n",
    "    'fold_results': [\n",
    "        {k: float(v) if isinstance(v, (np.floating, float)) else v \n",
    "         for k, v in result.items()} \n",
    "        for result in fold_results\n",
    "    ],\n",
    "    'summary': {\n",
    "        'eer': {'mean': float(mean_eer), 'std': float(std_eer)},\n",
    "        'accuracy': {'mean': float(mean_acc), 'std': float(std_acc)},\n",
    "        'auc': {'mean': float(mean_auc), 'std': float(std_auc)},\n",
    "        'precision': {'mean': float(mean_prec), 'std': float(std_prec)},\n",
    "        'recall': {'mean': float(mean_rec), 'std': float(std_rec)},\n",
    "        'f1': {'mean': float(mean_f1), 'std': float(std_f1)},\n",
    "        'loss': {'mean': float(mean_loss), 'std': float(std_loss)}\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(cv_results, f, indent=4)\n",
    "\n",
    "print(f\"\\n > [Info] Results saved to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889463bd",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# STEP 8: VISUALIZATION\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa1a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cross-validation metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Baseline DenseNet121 - Genuine vs Forged Classification Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "folds = list(range(1, N_FOLDS + 1))\n",
    "accuracies = avg_metrics['accuracy']\n",
    "precisions = avg_metrics['precision']\n",
    "recalls = avg_metrics['recall']\n",
    "f1_scores = avg_metrics['f1']\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0, 0].bar(folds, accuracies, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].axhline(y=mean_acc, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_acc:.4f}')\n",
    "axes[0, 0].set_xlabel('Fold', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0, 0].set_title('Accuracy per Fold', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_ylim([0, 1])\n",
    "axes[0, 0].set_xticks(folds)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Precision plot\n",
    "axes[0, 1].bar(folds, precisions, color='green', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].axhline(y=mean_prec, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_prec:.4f}')\n",
    "axes[0, 1].set_xlabel('Fold', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Precision', fontsize=12)\n",
    "axes[0, 1].set_title('Precision per Fold', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_ylim([0, 1])\n",
    "axes[0, 1].set_xticks(folds)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Recall plot\n",
    "axes[1, 0].bar(folds, recalls, color='orange', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].axhline(y=mean_rec, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_rec:.4f}')\n",
    "axes[1, 0].set_xlabel('Fold', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Recall', fontsize=12)\n",
    "axes[1, 0].set_title('Recall per Fold', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "axes[1, 0].set_xticks(folds)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# F1-Score plot\n",
    "axes[1, 1].bar(folds, f1_scores, color='purple', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].axhline(y=mean_f1, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_f1:.4f}')\n",
    "axes[1, 1].set_xlabel('Fold', fontsize=12)\n",
    "axes[1, 1].set_ylabel('F1-Score', fontsize=12)\n",
    "axes[1, 1].set_title('F1-Score per Fold', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "axes[1, 1].set_xticks(folds)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = os.path.join(CHECKPOINT_DIR, 'cv_metrics.png')\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\" > [Info] Cross-validation metrics plot saved to: {plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55806980",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# VALIDATION SUMMARY\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CROSS-VALIDATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: Baseline DenseNet121 (Binary Classification)\")\n",
    "print(f\"Task: Genuine vs Forged Signature Detection\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"   - Image Size: {INPUT_SHAPE}\")\n",
    "print(f\"   - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   - Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   - Epochs: {EPOCHS}\")\n",
    "print(f\"   - K-Folds: {N_FOLDS}\")\n",
    "print(f\"   - Classes: 2 (0=Genuine, 1=Forged)\")\n",
    "print(f\"   - Pretrained Weights: {os.path.basename(PRETRAINED_WEIGHTS_PATH)}\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"   - Training Users: {len(train_users)}\")\n",
    "print(f\"   - Test Users: {len(test_users)}\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"   - Mean Accuracy: {mean_acc:.4f} ({mean_acc*100:.2f}%)\")\n",
    "print(f\"   - Mean F1-Score: {mean_f1:.4f}\")\n",
    "print(f\"   - Mean Precision: {mean_prec:.4f}\")\n",
    "print(f\"   - Mean Recall: {mean_rec:.4f}\")\n",
    "print(f\"\\nSaved Artifacts:\")\n",
    "print(f\"   - Checkpoints: {CHECKPOINT_DIR}\")\n",
    "print(f\"   - Results JSON: {results_file}\")\n",
    "print(f\"   - Metrics Plot: {plot_path}\")\n",
    "print(f\"\\nBest Models per Fold:\")\n",
    "for fold in range(N_FOLDS):\n",
    "    model_path = os.path.join(CHECKPOINT_DIR, f\"best_model_fold_{fold}.pth\")\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"   - Fold {fold+1}: {os.path.basename(model_path)}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nCross-Validation Complete! ✓\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
